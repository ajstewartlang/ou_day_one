---
title: "Day One"
author: "[Andrew Stewart](https://ajstewartlang.netlify.app/)"
date: "July 2021"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

# Your First R Script

The first thing we're going to do is to run a script that will generate some data visualisations based on an open dataset of UFO observations. The dataset is part of Tidy Tuesday data wrangling/visualisation set of challenges. The script below contains code that wranlges and visualises the data. Don't worry at this stage what each line does as we'll be exploring it in more detail later. At this point, I just want you all to run a script to check your installation of R is working as it should. And I thought it might as well be a fun script!

We will create three visualisations of UFO sightings in the US using a database of more than 80,000 UFO sightings over the years.

The first two lines below load the packages we are going to use. The `tidyverse` package contains a number of individual packages that all play nicely together. They are based on the idea of tidy data and functions that operate on the data in a human-readable manner. 

```{r}
library(tidyverse) 
```

Now we are going to read in the data. Note that we are doing this using the `read_csv()` function but than than reading in the data locally (i.e., from somewhere on our own computer), we are loading it from a website. The data is in `.csv.` format which is a common format for rectangular datasets (i.e., datasets with rows and columns). The line at the start `# read in data` is a comment - as indicated by the `#` and won't be interpreted as an R command. Comments are useful not just for others but for your future self. It's a good idea to get into the habit of adding comments to your code - your future self will be thankful!
 
```{r}
# read in data 
ufo_sightings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv")
```

Next we're going to create our first plot. We map the plot itself onto a variable I'm calling `plot1`. To display the plot, we can then simply type `plot1`. This visualisation shows us the top 10 states with the number of UFO sightings in each state. The symbol `%>%` in the code below is the pipe operator and can be read as "and then". It allows us to pipe together lines of code. The `+` can be read in the same way when it appears at the end of lines following the `ggplot()` function.

```{r}
# plot of top 10 US states with number of sightings in each state
plot1 <- ufo_sightings %>%
  filter(!is.na(state)) %>%
  mutate(state = str_to_upper(state)) %>%
  group_by(state) %>%
  tally() %>%
  top_n(10) %>%
  ggplot(aes(x = reorder(state, n), y = n, fill = state)) +
  geom_col() + 
  coord_flip() +
  guides(fill = FALSE) + 
  labs(title = "Top 10 States for UFO Sightings",
       x = NULL, 
       y = NULL) +
  ylim(0, 11000) +
  theme_minimal() +
  theme(text = element_text(size = 15))

plot1
```

The next chunk of code involves some data wrangling to figure out the top 10 states for UFO sightings and save those names to a variable we're calling `top_states`. If you then type the variable name `top_states` you'll see the abbreviations of those 10 state names printed out.

```{r}
# work out the top 10 states with most UFO sightings
top_states <- ufo_sightings %>%
  filter(!is.na(state)) %>%
  group_by(state) %>%
  tally() %>%
  top_n(10) %>% 
  pull(state)

top_states
```

We're now going to do a little wrangling to ensure we select the 48 contiguous states (sorry Alaska and Hawaii). I'm mapping this list of 48 states onto a new variable called `tidied_ufo`.

```{r}
# work out states within lat and long limits (i.e., exclude Alaska)
tidied_ufo <- ufo_sightings %>%
  filter(country == "us") %>%
  filter(latitude > 24 & latitude < 50)
```

Now we're ready for our second plot where we're plotting the sightings using their latitude and longitude co-ordinates. There are so many we have a pretty decent approximation of the contiguous US.

```{r}
# plot all sightings on a map of the US
plot2 <- tidied_ufo %>%
  ggplot(aes(x = longitude, y = latitude)) + 
  geom_point(size = .5, alpha = .25) +
  theme_void() +
  coord_cartesian() +
  labs(title = "Sites of UFO Sightings in the US") +
  guides(colour = FALSE) +
  guides(fill = FALSE) +
  theme(text = element_text(size = 15))
 
plot2
```

There are quite a lot of sightings in California (maybe not a huge surprise) so let's look at those separately. We'll start by plotting the 10 most common UFO shapes seen in the state.

```{r}
# plot of top 10 UFO shapes in California
plot3 <- tidied_ufo %>%
  filter(state == "ca") %>%
  filter(ufo_shape != "other") %>%
  filter(ufo_shape != "unknown") %>%
  group_by(ufo_shape) %>%
  tally() %>%
  top_n(10) %>%
  mutate(ufo_shape = str_to_title(ufo_shape)) %>%
  ggplot(aes(x = reorder(ufo_shape, n), y = n, fill = ufo_shape)) +
  geom_col() + 
  coord_flip() +
  guides(fill = FALSE) + 
  labs(title = "California Top 10 UFO Shapes",
       x = NULL, 
       y = NULL) +
  theme_minimal() +
  theme(text = element_text(size = 15))

plot3
```

And if we want to save any of our plots we could do so as follows... 

```{r}
ggsave("ufo_plot.jpg", plot = plot2, width = 12, height = 10)
```

## Breakout Rooms

In your groups I'd like you to change some parameters in the script above to see what happens. Maybe think about changing the alpha value in `geom_point()` What does that do? What other values could you change in the script?

# Data Wrangling

Let's take our first detailed look at data wrangling. We are going to start with a dataset that comes with the `tidyverse`. The dataset is called `mpg` and comprises fuel economy data from 1999 to 2008 for 38 popular models of cars in the US.

You should create a new script for this lesson. At the start of the new script, add the line `library(tidyverse)` to ensure that the `tidyverse` is loaded when you run your script.

## The `mpg` dataset

The `mpg` dataset is a dataset that comes with the `tidyverse`. In the help file, which you can access by typing `help(mpg)` or `?mpg` we see the following:

>Description
>This dataset contains a subset of the fuel economy data that the EPA makes available on http://fueleconomy.gov. It contains only models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car. 

>A data frame with 234 rows and 11 variables.  

>manufacturer - manufacturer  
model - model name  
displ - engine displacement, in litres  
year - year of manufacture  
cyl - number of cylinders  
trans -type of transmission  
drv -f = front-wheel drive, r = rear wheel drive, 4 = 4wd  
cty - city miles per gallon  
hwy - highway miles per gallon  
fl - fuel type  
class - "type" of car  

## Using `head()` and `str()`

We can explore the `mpg` dataset in a number of ways. If we want to look at the first 6 lines of the dataset, we can use the `head()` function.

```{r}
head(mpg)
```

We see that it is a tibble - or a rectangular data frame - made up of rows and columns. This is `tidy` format where each observation corresponds to a row. Most of the analyses we will run in R involve `tidy` data. Within the `tidyverse`, the `tibble` is the standard way to represent data. You'll spend a lot of time tidying and wrangling your data to get it into this format. By doing this in R using a script that you write, you are making this key stage *reproducible*. You can run the script again on an updated or different dataset - thus likely saving you lots of time...

We can also ask for information about the structure of our dataset with `str()`. This will tell us about the columns, what type of variable each is, the number of rows etc.

```{r}
str(mpg)
```

## Use `select()` to select columns

If we want to, we could just select one of the columns using the `select()` function. Below we are just selecing the column entitled `manufacturer`.

```{r}
mpg %>%
  select(manufacturer)
```

Related to the `select()` function is `rename()`. It does exactly what you think it might; it renames a column.

We can also look at the different car manufacturers in the dataset by using the `distinct()` function. This gives us the unique manufacturer names. This function can be quite handy if you want to check a dataset for duplicates of (e.g.) participant IDs.

```{r}
mpg %>%
  distinct(manufacturer)
```

## Use `filter()` to select rows

Sometimes we might want to select only a subset of rows in our dataset. We can do that using the `filter()` function. For example, here we filter our dataset to include only cars made by 'honda'.

```{r}
mpg %>%
  filter(manufacturer == "honda")
```

Note, we use the operator `==` which means 'is equal to'. This is a logical operator - other logical operators include less than `<`, greater than `>`, less than or equal to `<=`, greater then or equal to `>=`, and is not equal to `!=`.

We can also filter using a combination of possibilities via logical OR `|` or logical AND `&`. The first code chunk below filters the dataset for cases where the manufacturer is 'honda' OR 'toyota'.

```{r}
mpg %>%
  filter(manufacturer == "honda" | manufacturer == "toyota")
```

While below we filter for cases where the manufacturer is 'honda' and the year of manufacture is '1999'.

```{r}
mpg %>% 
  filter(manufacturer == "honda" & year == "1999")
```

### Combining functions

We can combine the use of `filter()` with `select()` to filter for case where the manufacturer is 'honda', the year of manufacture is '1999' and we only want to display these two columns plus those telling us about fuel economy - `cty` and `hwy`.

```{r}
mpg %>% 
  filter(manufacturer == "honda" & year == "1999") %>%
  select(manufacturer, year, cty, hwy)
```

By combining just a few functions, you can imagine that we can build some quite complex data wrangling rules quite straightforwardly.  

## The pipe `%>%`

Note that in these examples above we used the `%>%` operator again. All of the functions (such as `select()`, `filter()` etc.) in the `tidyverse` are known as verbs, and they describe what they do. The pipe is one of the most commonly used operators in the `tidyverse` and allows us to chain together different lines of code - with the output of each line being passed on as input into the next. In this example, the dataset `mpg` is passed along to the `distinct()` function where we ask for a list of the distinct (i.e., unique) manufacturers. This output itself is a vector. Vectors are a basic data structure and contain elements of the same type - for example, a bunch of numbers. We can add another line to our piped chain to tell us how many elements are in this vector. We could read this out loud as 'take the dataset mpg, and then work out the distinct manufacturer names, and then count them'. 

```{r}
mpg %>% 
  distinct(manufacturer) %>%
  count()
```

## Tidying up a dataset

### Tidying variable names

At the moment, the car manufacturer names are all in lower case. It would look a lot nicer if they were in title case (i.e., with capitalisation on the first letter of each word). We can use the `mutate()` function to create a new column - this time, the name of the new column is also the name of the old column that we're wanting to modify using the function `str_to_title()`. What this will do is overwrite the column `manufacturer` and replace it with the new version with the car manufacturer names in title case.

```{r}
mpg %>%
  mutate(manufacturer = str_to_title(manufacturer)) 
```

The column `model` is also lowercase. Let's make that title case too.  We can use the `mutate()` function to work over more than one column at the same time like this:

```{r}
mpg %>%
  mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model))
```

There are quite a few columns there, so how about we select just the manufacturer, model, year, transmission, and hwy columns:

```{r}
mpg %>%
  mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model)) %>%
  select(manufacturer, model, year, trans, hwy)
```

## Recoding variables 

In the real world, data frames do not always arrive on our computer in tidy format. Very often you need to engage in some data tidying before you can do anything useful with them. We're going to look at an example of how we go from messy data to tidy data.

```{r, message=FALSE}
my_messy_data <- read_csv("https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/my_data.csv")
```

We ran a reaction time experiment with 24 participants and 4 conditions - the conditions are numbered 1-4 in our data file.

```{r}
head(my_messy_data)
```

This is a repeated measures design where we had one factor (Prime Type) with two levels (A vs. B) and a second factor (Target Type) with two levels (A vs. B). We want to recode our data frame so it better matches our experimental design. First we need to recode our 4 conditions like this:

Recode condition columns follows:
Condition 1 = Prime A, Target A
Condition 2 = Prime A, Target B
Condition 3 = Prime B, Target A
Condition 4 = Prime B, Target B

```{r}
my_messy_data %>% 
  mutate(condition = recode(condition,
                            "1" = "PrimeA_TargetA",
                            "2" = "PrimeA_TargetB", 
                            "3" = "PrimeB_TargetA", 
                            "4" = "PrimeB_TargetB")) %>%
  head()
```

We now need to separate out our Condition column into two - one for our first factor (Prime), and one for our second factor (Target). The `separate()` function does just this - when used in conjunction with a piped tibble, it needs to know which column we want to separate, what new columns to create by separating that original column, and on what basis we want to do the separation. In the example below we tell `separate()` that we want to separate the column labeled `condition` into two new columns called `Prime` and `Target` and we want to do this at any points where a `_` is present in the column to be separated.

```{r}
my_messy_data %>% 
  mutate(condition = recode(condition,
                            "1" = "PrimeA_TargetA",
                            "2" = "PrimeA_TargetB", 
                            "3" = "PrimeB_TargetA", 
                            "4" = "PrimeB_TargetB")) %>%
  separate(col = "condition", into = c("Prime", "Target"), sep = "_")
```

```{r}
my_messy_data %>% 
  mutate(condition = recode(condition,
                            "1" = "PrimeA_TargetA",
                            "2" = "PrimeA_TargetB", 
                            "3" = "PrimeB_TargetA", 
                            "4" = "PrimeB_TargetB")) %>%
  separate(col = "condition", into = c("Prime", "Target"), sep = "_") %>%
  mutate(Prime = factor(Prime), Target = factor(Target))
```

## The pivot functions

Most of the analysis we will conduct in R requires our data to be in tidy, or long, format. In such data sets, one row corresponds to one observation. In the real world, data are rarely in the right format for analysis. In R, the `pivot_wider()` and `pivot_longer()` functions are designed to reshape our data files. First, let's load a datafile that is in wide format (i.e., multiple observations per row). It is from an experiment where we had four conditions (labelled Condition1, Condition2, Condition3, and Condition4). In addition to there being a column for each of the 4 conditions, we also have a column corresponding to participant ID. Each cell in the data set corresponds to a reaction time (measured in milliseconds).

```{r, message=FALSE}
my_wide_data <- read_csv("https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/my_wide_data.csv")
```

### The `pivot_longer()` function

```{r}
head(my_wide_data)
```

So, we can see the data file is in wide format. We want to reshape it to long format. We can do that using the `pivot_longer()` function.

Minially, we need to specify the data frame that we want to reshape, the columns that we want to 'pivot' into longer format, the name of the new column that we are creating, and the name of the column that will hold the values of our reshaped data frame. We are going to map the output to a variable I'm calling `my_longer_data`.

```{r}
my_longer_data <- my_wide_data %>%
  pivot_longer(cols = c(Condition1, Condition2, Condition3, Condition4), 
               names_to = "Condition", 
               values_to = "RT")
```

Now let's have a look at what our reshaped data frame looks like.

```{r}
head(my_longer_data)
```

So you can see our data are now in long - or tidy - format with one observation per row. Note that our `Condition` column isn't coded as a factor. It's important that our data set reflects the structure of our experiment so let's convert that column to a factor - note that in the following code we are now 'saving' the change as we are not mapping the output onto a variable name.

```{r}
my_longer_data %>%
  mutate(Condition = factor(Condition)) %>%
  head()
```

### The `pivot_wider()` function

We can use the `pivot_wider()` function to reshape a long data frame so that it goes from long to wide format. It works similarly to `pivot_longer()`. Let's take our new, long, data frame and turn it back into wide format. With `pivot_wider()` we minimally need to specify the data frame that we want to reshape, and a pair or arguments (names_from and values_from) that describe from which column to get the name of the output column, and from which column to get the cell values.  

```{r}
my_wider_data <- my_longer_data %>%
  pivot_wider(names_from = "Condition", 
              values_from = "RT")
```

We can check that our data set is back in wide format.

```{r}
head(my_wider_data)
```

# Summarising Data

Once a dataset has been tidied, often one of the first things we want to do is generate summary statistics. We'll continue using the `mpg` dataset. How would be go about generating (e.g.) the means and standard deviations grouped by car manufacturer for one of our variables?

## Using `group_by()` and `summarise()`

We are going to use the `group_by()` function to group the dataset, and then the `summarise()` function to calculate the mean and sd of the `hwy` variable. The `summarise()` function can take a lot of different functions to give us summary statistics. To read more about the different options, type `?summarise` in the Console window. Commonly used ones are `mean()`, `median()`, `sd()`.

```{r}
mpg %>%
  group_by(manufacturer) %>%
  summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n())
```
Note that this output is currently ordered alphabetically by the first column `manufacturer`. What if we wanted to order this out by mean highway fuel economy highest (best) to lowest (worst)? We can use the arrange function.

### Re-ordering the output with `arrange()`

```{r}
mpg %>%
  group_by(manufacturer) %>%
  summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %>%
  arrange(mean_hwy)
```

Hmm, so that isn't what we want - this is going from lowest to highest which is the default in R. We can change that by putting a `-` sign in from of the parameter we can to order by.

```{r}
mpg %>%
  group_by(manufacturer) %>%
  summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %>%
  arrange(-mean_hwy)
```

This is looking better.

## Adding columns using `mutate()`

We can add a new column that I'm calling `mean_hwy` using the `mutate()` function like this.

```{r}
mpg %>% 
  group_by(manufacturer) %>%
  mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy))
```

We have too many columns to display on this page so we can drop a coouple by using the `select()` function slightly differently. By putting a `-` sign in front of a column names in `select()` we end up dropping it.

```{r}
mpg %>% 
  group_by(manufacturer) %>%
  mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) %>%
  select(-class, -trans)
```

Note that this doesn't change the `mpg` dataset permanently - the changes won't be saved unless we map the output of this code onto a new variable. Below I am doing this by using the assignment operator `<-` to map it onto a new variable I'm calling `mpg_with_mean`. Note that we remove the grouping at the end as we don't want our grouping rule to remain in our new data frame.

```{r}
mpg_with_mean <- mpg %>% 
  group_by(manufacturer) %>%
    mutate(mean_hwy = mean(hwy), sd_hyw = sd(hwy)) %>%
  ungroup() %>%
  select(-class, -trans) 
```

We can then inspect this new variable using `head()` and `str()`.

```{r}
head(mpg_with_mean)
```

```{r}
str(mpg_with_mean)
```

# Your challenge

The `tidyverse` has a number of other built-in data sets. Another one is the `starwars` data set. You can have a look at it by typing `starwars` or by typing `view(starwars)`. This second option will open the data set in a new window. Have a go playing around with it. Work out the mean height of humans in the Star Wars universe. There might be some missing data (indicated by `NA`). You can use the `na.rm = TRUE` parameter in your `summarise()` function to ignore these values when generating your summary statistics. 

Another way to filter out `NA` values is to use the `filter()` function in your pipeline. The function `is.na()` returns a logical value of TRUE of FALSE. The operator `!` means NOT, so the expression `!is.na(height)` will return TRUE when the height value is present, and FALSE if absent. By combining this with `filter()` we have the line `filter(!is.na(height))` which will filter only the cases where we have height data (i.e., `!is.na(height)` is TRUE). So your code might look like this:

```{r, eval=FALSE}
starwars %>%
  filter(!is.na(height)) %>%
  filter(species == "Human") %>%
  summarise(mean_height = mean(height))
```

Replace the word `mean` in the `summarise()` line with `median`. What other things can you replace it with? Hint: type `?summarise` in the console.  What other summary information can you extract from this dataset?